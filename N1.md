# N1_basic Solution Overview

## Task 1.1 – Agent Implementations
- **Epsilon‑greedy (`eg_select`, `eg_update`)** lazily initializes value/counter dictionaries for every arm present in the current state, flips a Bernoulli coin with probability `epsilon` to explore uniformly, and otherwise takes the deterministic argmax over the currently available `Q` estimates. Updates validate the inputs, then either apply the sample‑average rule (`alpha is None`) or the constant step size rule, keeping counts in sync for dynamic action sets.
- **UCB (`ucb_select`, `ucb_update`)** performs the same lazy initialization but forces each new arm to be played once before switching to the optimism‑in‑the‑face‑of‑uncertainty index `Q[a] + c * sqrt(log t / N[a])`. The update routine increments the global time step, arm count, and recomputes the sample average, again guarding against invalid arms or states.
- **Gradient Bandit (`gb_select`, `gb_update`)** maintains softmax preferences `H`. Selection initializes unseen arms with zero preference, builds the softmax distribution over the currently available IDs, and samples via the agent RNG. The update step enforces the interface requirements, computes the gradient step `(reward - baseline) * (1{a=i} - π(a))` over the actions in the present state, and refreshes the running reward baseline `avg_reward` and timestep.
- Each agent’s implementation is patched onto the class and verified with the dedicated pytest suites, ensuring interface contracts and dynamic arm handling behave as specified in `tests/test_bandits_*.py`.

## Task 1.1 – Visual Sanity Check
- Using `experiment_factory` with `MABTestEnvironment`, the notebook compares Random, ε‑greedy (`ε=0.1`), UCB (`c=1.5`), and Gradient Bandit (`α=0.1`) over 100 seeds and 500 steps.
- The plotted mean rewards with min–max bands show the expected ordering: Random is worst, ε‑greedy and Gradient improve steadily, and UCB quickly finds the best arm thanks to its exploration bonus.

## Task 1.2 – Interpretation
- **Q1.2.1**: After aggregating ≥50 seeds, UCB (`c=1.5`) achieves the highest long‑run average reward, so `best = 3` (corresponding to “UCB performs best”).
- **Q1.2.2**: The early UCB spike is caused by the optimism bonus forcing fast exploration; once counts grow, the bonus shrinks and the average reward stabilizes. Decreasing `c` to 1 would shrink that bonus and therefore make the spike less pronounced (`less_prominent = True`).

These steps fully solve the N1 notebook: the agents follow the specification, pass tests, and their behavior is validated and interpreted through experiments.
