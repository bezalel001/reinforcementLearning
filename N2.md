# N2_hyperparameter_optimization Solution Overview

## Task 2.1 – Optimization Setups
- **Criterion (2.1.1)**: compare agents by the average reward per step (mean click rate) aggregated over all seeds and steps; this is the metric plotted by `plot_reward_band`.
- **ε‑greedy search (2.1.2)**: `your_epsilon_greedy_optimization` sweeps `epsilon ∈ {0.05, 0.1, 0.2}` and optional constant step sizes `alpha ∈ {None, 0.05, 0.1}` using 30 seeds, 2 000 steps, and the Recsys environment at 30 users/minute. Each configuration is run via `experiment_factory`, compared against a Random baseline, and visualized with reward bands. The function reports the best combination (ε=0.05, α=0.1 in the final comparison) together with its mean reward, highlighting that higher ε boosts exploration while constant α improves adaptation.
- **UCB search (2.1.3)**: `your_UCB_optimization` evaluates `c ∈ {0.5, 1.0, 1.5, 2.0}` under the same protocol. Rewards are logged per configuration, the plot juxtaposes the curves, and the best performer (`c≈1.0` within this coarse grid) is printed. Lower `c` under-explores; higher `c` causes an initial spike and potentially wastes pulls on stale articles.
- **Gradient Bandit search (2.1.4)**: `your_gradient_bandit_optimization` tests learning rates `α ∈ {0.02, 0.05, 0.1, 0.2}`. Again, each agent is built through `experiment_factory`, evaluated over 30 seeds, and plotted versus a Random baseline. The routine returns the best α (≈0.05 here), noting the adaptation/variance trade-off.

## Task 2.2 – Comparing Best Configurations
- The chosen hyperparameters (`ε=0.05, α=0.1`, `c=1.0`, `α=0.05` for Gradient Bandit) are plugged into a final experiment block to compare all tuned agents plus the Random baseline with identical evaluation settings. The resulting figure illustrates their relative performance after tuning.

## Task 2.3 – Non-stationarity Check
- `your_non_stationary_optimization` contrasts ε‑greedy with sample-average updates vs. a constant step size (`α=0.1`). The constant step-size version attains a slightly higher mean reward, confirming that the recsys environment’s reward distribution drifts (because article freshness and catalog updates change click probabilities). The accompanying markdown explicitly states this finding.

## Task 2.4 – Hyperparameter Optimization in the Real World
- The final answer explains that, without a simulator, hyperparameters must be tuned using logged interaction data (through IPS/DR counterfactual estimators) and carefully controlled online A/B tests, since live traffic is the only feedback source.

Combined, these sections document the optimization strategy, provide runnable sweeps for each agent, interpret the results, and answer the conceptual questions required by N2.
